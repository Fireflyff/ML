{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469ecc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from gensim import corpora,models,similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c020267",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ['工业互联网平台的核心技术是什么',\n",
    "            '工业现场生产过程优化场景有哪些',\n",
    "            '互联网泡沫即将过去']\n",
    "text3 = \"大厂场景固定，泡沫比较少\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9f313b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/dm/n5w6s3vs0yv6nm2h3smdvs380000gn/T/jieba.cache\n",
      "Loading model cost 0.803 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['工业', '互联网', '平台', '的', '核心技术', '是', '什么'],\n",
       " ['工业', '现场', '生产', '过程', '优化', '场景', '有', '哪些'],\n",
       " ['互联网', '泡沫', '即将', '过去']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_cut(doc):\n",
    "    seg = [jieba.lcut(w) for w in doc]\n",
    "    return seg\n",
    "\n",
    "texts= word_cut(documents)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb433540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],\n",
       " {'互联网': 0,\n",
       "  '什么': 1,\n",
       "  '工业': 2,\n",
       "  '平台': 3,\n",
       "  '是': 4,\n",
       "  '核心技术': 5,\n",
       "  '的': 6,\n",
       "  '优化': 7,\n",
       "  '哪些': 8,\n",
       "  '场景': 9,\n",
       "  '有': 10,\n",
       "  '现场': 11,\n",
       "  '生产': 12,\n",
       "  '过程': 13,\n",
       "  '即将': 14,\n",
       "  '泡沫': 15,\n",
       "  '过去': 16})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为语料库中出现的所有单词分配了一个唯一的整数id\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.keys(), dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b5c88ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(2, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)],\n",
       " [(0, 1), (14, 1), (15, 1), (16, 1)]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过doc2bow转化为稀疏向量\n",
    "corpus=[dictionary.doc2bow(text)for text in texts]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16839d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = dictionary.doc2bow(jieba.lcut(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49ad488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65e1997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus],len(dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf313ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.26469827, 0.3992843 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index[tfidf[test_corpus]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3247b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['工业', '互联网', '平台', '的', '核心技术', '是', '什么'],\n",
       " ['工业', '现场', '生产', '过程', '优化', '场景', '有', '哪些'],\n",
       " ['互联网', '泡沫', '即将', '过去']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f47af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['大厂', '场景', '固定', '，', '泡沫', '比较', '少']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.lcut(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1b8aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#。word2vec\n",
    "context = [\n",
    "    \"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling.\",\n",
    "    \"We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ- ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor- mance on our test set for measuring syntactic and semantic word similarities.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab1f612e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  ' ',\n",
       "  'recently',\n",
       "  ' ',\n",
       "  'introduced',\n",
       "  ' ',\n",
       "  'continuous',\n",
       "  ' ',\n",
       "  'Skip',\n",
       "  '-',\n",
       "  'gram',\n",
       "  ' ',\n",
       "  'model',\n",
       "  ' ',\n",
       "  'is',\n",
       "  ' ',\n",
       "  'an',\n",
       "  ' ',\n",
       "  'efficient',\n",
       "  ' ',\n",
       "  'method',\n",
       "  ' ',\n",
       "  'for',\n",
       "  ' ',\n",
       "  'learning',\n",
       "  ' ',\n",
       "  'high',\n",
       "  '-',\n",
       "  'quality',\n",
       "  ' ',\n",
       "  'distributed',\n",
       "  ' ',\n",
       "  'vector',\n",
       "  ' ',\n",
       "  'representations',\n",
       "  ' ',\n",
       "  'that',\n",
       "  ' ',\n",
       "  'capture',\n",
       "  ' ',\n",
       "  'a',\n",
       "  ' ',\n",
       "  'large',\n",
       "  ' ',\n",
       "  'num',\n",
       "  '-',\n",
       "  ' ',\n",
       "  'ber',\n",
       "  ' ',\n",
       "  'of',\n",
       "  ' ',\n",
       "  'precise',\n",
       "  ' ',\n",
       "  'syntactic',\n",
       "  ' ',\n",
       "  'and',\n",
       "  ' ',\n",
       "  'semantic',\n",
       "  ' ',\n",
       "  'word',\n",
       "  ' ',\n",
       "  'relationships',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'In',\n",
       "  ' ',\n",
       "  'this',\n",
       "  ' ',\n",
       "  'paper',\n",
       "  ' ',\n",
       "  'we',\n",
       "  ' ',\n",
       "  'present',\n",
       "  ' ',\n",
       "  'several',\n",
       "  ' ',\n",
       "  'extensions',\n",
       "  ' ',\n",
       "  'that',\n",
       "  ' ',\n",
       "  'improve',\n",
       "  ' ',\n",
       "  'both',\n",
       "  ' ',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'quality',\n",
       "  ' ',\n",
       "  'of',\n",
       "  ' ',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'vectors',\n",
       "  ' ',\n",
       "  'and',\n",
       "  ' ',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'training',\n",
       "  ' ',\n",
       "  'speed',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'By',\n",
       "  ' ',\n",
       "  'subsampling',\n",
       "  ' ',\n",
       "  'of',\n",
       "  ' ',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'frequent',\n",
       "  ' ',\n",
       "  'words',\n",
       "  ' ',\n",
       "  'we',\n",
       "  ' ',\n",
       "  'obtain',\n",
       "  ' ',\n",
       "  'significant',\n",
       "  ' ',\n",
       "  'speedup',\n",
       "  ' ',\n",
       "  'and',\n",
       "  ' ',\n",
       "  'also',\n",
       "  ' ',\n",
       "  'learn',\n",
       "  ' ',\n",
       "  'more',\n",
       "  ' ',\n",
       "  'regular',\n",
       "  ' ',\n",
       "  'word',\n",
       "  ' ',\n",
       "  'representations',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'We',\n",
       "  ' ',\n",
       "  'also',\n",
       "  ' ',\n",
       "  'describe',\n",
       "  ' ',\n",
       "  'a',\n",
       "  ' ',\n",
       "  'simple',\n",
       "  ' ',\n",
       "  'alterna',\n",
       "  '-',\n",
       "  ' ',\n",
       "  'tive',\n",
       "  ' ',\n",
       "  'to',\n",
       "  ' ',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'hierarchical',\n",
       "  ' ',\n",
       "  'softmax',\n",
       "  ' ',\n",
       "  'called',\n",
       "  ' ',\n",
       "  'negative',\n",
       "  ' ',\n",
       "  'sampling',\n",
       "  '.'],\n",
       " ['We',\n",
       "  ' ',\n",
       "  'propose',\n",
       "  ' ',\n",
       "  'two',\n",
       "  ' ',\n",
       "  'novel',\n",
       "  ' ',\n",
       "  'model',\n",
       "  ' ',\n",
       "  'architectures',\n",
       "  ' ',\n",
       "  'for',\n",
       "  ' ',\n",
       "  'computing',\n",
       "  ' ',\n",
       "  'continuous',\n",
       "  ' ',\n",
       "  'vector',\n",
       "  ' ',\n",
       "  'repre',\n",
       "  '-',\n",
       "  ' ',\n",
       "  'sentations',\n",
       "  ' ',\n",
       "  'of',\n",
       "  ' ',\n",
       "  'words',\n",
       "  ' ',\n",
       "  'from',\n",
       "  ' ',\n",
       "  'very',\n",
       "  ' ',\n",
       "  'large',\n",
       "  ' ',\n",
       "  'data',\n",
       "  ' ',\n",
       "  'sets',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'The',\n",
       "  ' ',\n",
       "  'quality',\n",
       "  ' ',\n",
       "  'of',\n",
       "  ' ',\n",
       "  'these',\n",
       "  ' ',\n",
       "  'representations',\n",
       "  ' ',\n",
       "  'is',\n",
       "  ' ',\n",
       "  'measured',\n",
       "  ' ',\n",
       "  'in',\n",
       "  ' ',\n",
       "  'a',\n",
       "  ' ',\n",
       "  'word',\n",
       "  ' ',\n",
       "  'similarity',\n",
       "  ' ',\n",
       "  'task',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'and',\n",
       "  ' ',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'results',\n",
       "  ' ',\n",
       "  'are',\n",
       "  ' ',\n",
       "  'compared',\n",
       "  ' ',\n",
       "  'to',\n",
       "  ' ',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'previ',\n",
       "  '-',\n",
       "  ' ',\n",
       "  'ously',\n",
       "  ' ',\n",
       "  'best',\n",
       "  ' ',\n",
       "  'performing',\n",
       "  ' ',\n",
       "  'techniques',\n",
       "  ' ',\n",
       "  'based',\n",
       "  ' ',\n",
       "  'on',\n",
       "  ' ',\n",
       "  'different',\n",
       "  ' ',\n",
       "  'types',\n",
       "  ' ',\n",
       "  'of',\n",
       "  ' ',\n",
       "  'neural',\n",
       "  ' ',\n",
       "  'networks',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'We',\n",
       "  ' ',\n",
       "  'observe',\n",
       "  ' ',\n",
       "  'large',\n",
       "  ' ',\n",
       "  'improvements',\n",
       "  ' ',\n",
       "  'in',\n",
       "  ' ',\n",
       "  'accuracy',\n",
       "  ' ',\n",
       "  'at',\n",
       "  ' ',\n",
       "  'much',\n",
       "  ' ',\n",
       "  'lower',\n",
       "  ' ',\n",
       "  'computational',\n",
       "  ' ',\n",
       "  'cost',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'i',\n",
       "  '.',\n",
       "  'e',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'it',\n",
       "  ' ',\n",
       "  'takes',\n",
       "  ' ',\n",
       "  'less',\n",
       "  ' ',\n",
       "  'than',\n",
       "  ' ',\n",
       "  'a',\n",
       "  ' ',\n",
       "  'day',\n",
       "  ' ',\n",
       "  'to',\n",
       "  ' ',\n",
       "  'learn',\n",
       "  ' ',\n",
       "  'high',\n",
       "  ' ',\n",
       "  'quality',\n",
       "  ' ',\n",
       "  'word',\n",
       "  ' ',\n",
       "  'vectors',\n",
       "  ' ',\n",
       "  'from',\n",
       "  ' ',\n",
       "  'a',\n",
       "  ' ',\n",
       "  '1.6',\n",
       "  ' ',\n",
       "  'billion',\n",
       "  ' ',\n",
       "  'words',\n",
       "  ' ',\n",
       "  'data',\n",
       "  ' ',\n",
       "  'set',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'Furthermore',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'we',\n",
       "  ' ',\n",
       "  'show',\n",
       "  ' ',\n",
       "  'that',\n",
       "  ' ',\n",
       "  'these',\n",
       "  ' ',\n",
       "  'vectors',\n",
       "  ' ',\n",
       "  'provide',\n",
       "  ' ',\n",
       "  'state',\n",
       "  '-',\n",
       "  'of',\n",
       "  '-',\n",
       "  'the',\n",
       "  '-',\n",
       "  'art',\n",
       "  ' ',\n",
       "  'perfor',\n",
       "  '-',\n",
       "  ' ',\n",
       "  'mance',\n",
       "  ' ',\n",
       "  'on',\n",
       "  ' ',\n",
       "  'our',\n",
       "  ' ',\n",
       "  'test',\n",
       "  ' ',\n",
       "  'set',\n",
       "  ' ',\n",
       "  'for',\n",
       "  ' ',\n",
       "  'measuring',\n",
       "  ' ',\n",
       "  'syntactic',\n",
       "  ' ',\n",
       "  'and',\n",
       "  ' ',\n",
       "  'semantic',\n",
       "  ' ',\n",
       "  'word',\n",
       "  ' ',\n",
       "  'similarities',\n",
       "  '.']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_corpus = [jieba.lcut(word) for word in context]\n",
    "dict_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aaeb809",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2c_model = models.Word2Vec(dict_corpus,window=50,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b5920e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('simple', 0.14790154993534088),\n",
       " ('Skip', 0.13093295693397522),\n",
       " ('called', 0.12758195400238037),\n",
       " ('data', 0.12099821120500565),\n",
       " ('several', 0.10152021795511246),\n",
       " ('vector', 0.09153813868761063),\n",
       " ('lower', 0.08061347156763077),\n",
       " ('very', 0.08015013486146927),\n",
       " ('subsampling', 0.07660150527954102),\n",
       " ('billion', 0.07590745389461517)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2c_model.wv.most_similar(\"vectors\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c49aa69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00899808,  0.02343618, -0.00635184, -0.0005505 , -0.01821155,\n",
       "       -0.04638669,  0.01477248,  0.05894374, -0.02766863, -0.03499538,\n",
       "       -0.00179536, -0.03759007, -0.01116655,  0.02498997,  0.0157638 ,\n",
       "       -0.00670933,  0.01024574, -0.00571091, -0.02050687, -0.05088061,\n",
       "        0.02079174,  0.01903727,  0.01672825,  0.00754327,  0.0146605 ,\n",
       "        0.00693148, -0.01916442, -0.01884183, -0.02243941,  0.00109882,\n",
       "        0.0171978 , -0.00439178,  0.0317363 , -0.04200909, -0.00687773,\n",
       "        0.03272486,  0.01748733, -0.00990182, -0.00228411, -0.03727836,\n",
       "       -0.01279959, -0.00306173, -0.01402469, -0.00011032,  0.01454152,\n",
       "       -0.00794416, -0.03582023,  0.00314365,  0.01354054,  0.01356626,\n",
       "       -0.00491282, -0.01408146, -0.01526404, -0.01050801,  0.01041296,\n",
       "        0.00497275,  0.01763285, -0.01106232, -0.02070753,  0.02627723,\n",
       "        0.01662021,  0.00347114,  0.0165413 ,  0.0059672 , -0.0120734 ,\n",
       "        0.02825534,  0.0172846 ,  0.03739831, -0.02354981,  0.03140558,\n",
       "        0.00824349,  0.02712493,  0.01147365, -0.00170408,  0.04309678,\n",
       "        0.01178291,  0.00588999,  0.00114649, -0.01553211, -0.0139434 ,\n",
       "       -0.0143456 ,  0.00268017, -0.01261577,  0.03595593, -0.02838829,\n",
       "       -0.00208241,  0.04127583,  0.01441189,  0.01657721,  0.00539335,\n",
       "        0.03763252,  0.0204954 ,  0.01405661,  0.00605675,  0.03369928,\n",
       "        0.0175342 ,  0.00062295, -0.01889437, -0.00500644,  0.01191556],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2c_model.wv[\"vectors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a769df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from gensim import corpora,models,similarities\n",
    "from collections import defaultdict   #用于创建一个空的字典，在后续统计词频可清理频率少的词语\n",
    "#1、读取文档\n",
    "doc1=\"./d1.txt\"\n",
    "doc2=\"./d2.txt\"\n",
    "d1=open(doc1,encoding='GBK').read()\n",
    "d2=open(doc2,encoding='GBK').read()\n",
    "#2、对要计算的文档进行分词\n",
    "data1=jieba.cut(d1)\n",
    "data2=jieba.cut(d2)\n",
    "#3、对分词完的数据进行整理为指定格式\n",
    "data11=\"\"\n",
    "for i in data1:\n",
    "    data11+=i+\" \"\n",
    "data21=\"\"\n",
    "for i in data2:\n",
    "    data21+=i+\" \"\n",
    "documents=[data11,data21]\n",
    "texts=[[word for word in document.split()] for document in documents]\n",
    "#4、 计算词语的频率\n",
    "frequency=defaultdict(int)\n",
    "for text in texts:\n",
    "    for word in text:\n",
    "        frequency[word]+=1\n",
    "'''\n",
    "#5、对频率低的词语进行过滤（可选）\n",
    "texts=[[word for word in text if frequency[word]>10] for text in texts]\n",
    "'''\n",
    "#6、通过语料库将文档的词语进行建立词典\n",
    "dictionary=corpora.Dictionary(texts)\n",
    "dictionary.save(\"./dict.txt\")    #可以将生成的词典进行保存\n",
    "#7、加载要对比的文档\n",
    "doc3=\"./d3.txt\"\n",
    "d3=open(doc3,encoding='GBK').read()\n",
    "data3=jieba.cut(d3)\n",
    "data31=\"\"\n",
    "for i in data3:\n",
    "    data31+=i+\" \"\n",
    "#8、将要对比的文档通过doc2bow转化为稀疏向量\n",
    "new_xs=dictionary.doc2bow(data31.split())\n",
    "#9、对语料库进一步处理，得到新语料库\n",
    "corpus=[dictionary.doc2bow(text)for text in texts]\n",
    "#10、将新语料库通过tf-idf model 进行处理，得到tfidf\n",
    "tfidf=models.TfidfModel(corpus)\n",
    "#11、通过token2id得到特征数\n",
    "featurenum=len(dictionary.token2id.keys())\n",
    "#12、稀疏矩阵相似度，从而建立索引\n",
    "index=similarities.SparseMatrixSimilarity(tfidf[corpus],num_features=featurenum)\n",
    "#13、得到最终相似结果\n",
    "sim=index[tfidf[new_xs]]\n",
    "print(sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
